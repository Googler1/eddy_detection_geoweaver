[{
  "history_id" : "hvhezya5p4c",
  "history_input" : "# Importing the required libraries for eddy workflow\nfrom eddy_import import *\nimport os\nfrom datetime import datetime\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom py_eddy_tracker import data\nfrom py_eddy_tracker.dataset.grid import RegularGridDataset",
  "history_output" : "",
  "history_begin_time" : 1665180984628,
  "history_end_time" : 1665181356947,
  "history_notes" : null,
  "history_process" : "6gs3ym",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ei27qagaqa0",
  "history_input" : "# Defining the start_axes, update_axes, plot_variabe  and setting the paths for eddy workflow\nfrom eddy_import import *\n\ndef start_axes(title):\n    fig = plt.figure(figsize=(13, 5))\n    ax = fig.add_axes([0.03, 0.03, 0.90, 0.94])\n    ax.set_aspect(\"equal\")\n    ax.set_title(title, weight=\"bold\")\n    return ax\n\n\ndef update_axes(ax, mappable=None):\n    ax.grid()\n    if mappable:\n        plt.colorbar(mappable, cax=ax.figure.add_axes([0.94, 0.05, 0.01, 0.9]))\n\n\ndef plot_variable(grid_object, var_name, ax_title, **kwargs):\n    ax = start_axes(ax_title)\n    m = grid_object.display(ax, var_name, **kwargs)\n    update_axes(ax, m)\n    ax.set_xlim(grid_object.x_c.min(), grid_object.x_c.max())\n    ax.set_ylim(grid_object.y_c.min(), grid_object.y_c.max())\n    return ax, m\n\ndata_root = os.path.join(os.path.expanduser(\"~\"), \"ML_eddies\")\ntrain_folder = os.path.join(data_root, \"cds_ssh_1998-2018_10day_interval\")\ntest_folder = os.path.join(data_root, \"cds_ssh_2019_10day_interval\")\n\nexample_file = os.path.join(test_folder, \"dt_global_twosat_phy_l4_20190101_vDT2021.nc\")\ndate = datetime(2019, 1, 1)\ng = RegularGridDataset(example_file, \"longitude\", \"latitude\")",
  "history_output" : "We assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\n",
  "history_begin_time" : 1665180991954,
  "history_end_time" : 1665181356971,
  "history_notes" : null,
  "history_process" : "23nut7",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qoz4pzyfola",
  "history_input" : "# setting the vmin and vmax using the eddy 'plot_variable' method\nfrom eddy_paths import *\nfrom copy import deepcopy\n\nax, m = plot_variable(\n    g,\n    \"adt\",\n    f\"ADT (m) before high-pass filter\",\n    vmin=-0.15,\n    vmax=0.15,\n)\nwavelength_km = 700\ng_filtered = deepcopy(g)\ng_filtered.bessel_high_filter(\"adt\", wavelength_km)\nax, m = plot_variable(\n    g_filtered,\n    \"adt\",\n    f\"ADT (m) filtered (Final: {wavelength_km} km)\",\n    vmin=-0.15,\n    vmax=0.15,\n)\nplt.show()\n",
  "history_output" : "Running",
  "history_begin_time" : 1665180996807,
  "history_end_time" : 1665181356977,
  "history_notes" : null,
  "history_process" : "zr8vzj",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gu1gryri26e",
  "history_input" : "# Dependencies Installation python code\npip install torch torchvision torchaudio matplotlib tensorboard torchmetrics seaborn opencv-python tqdm pandas\n",
  "history_output" : "Collecting torch\n  Downloading torch-1.12.1-cp39-none-macosx_10_9_x86_64.whl (133.8 MB)\nCollecting torchvision\n  Downloading torchvision-0.13.1-cp39-cp39-macosx_10_9_x86_64.whl (1.3 MB)\nCollecting torchaudio\n  Downloading torchaudio-0.12.1-cp39-cp39-macosx_10_9_x86_64.whl (3.1 MB)\nRequirement already satisfied: matplotlib in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (3.5.1)\nCollecting tensorboard\n  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\nCollecting torchmetrics\n  Downloading torchmetrics-0.10.0-py3-none-any.whl (529 kB)\nRequirement already satisfied: seaborn in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (0.11.2)\nRequirement already satisfied: opencv-python in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (4.6.0.66)\nRequirement already satisfied: tqdm in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (4.64.0)\nRequirement already satisfied: pandas in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\nRequirement already satisfied: typing-extensions in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.1.1)\nRequirement already satisfied: numpy in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.20.3)\nRequirement already satisfied: requests in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.27.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (9.0.1)\nRequirement already satisfied: cycler>=0.10 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\nRequirement already satisfied: packaging>=20.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: fonttools>=4.22.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.25.0)\nRequirement already satisfied: markdown>=2.6.8 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from tensorboard) (3.3.4)\nRequirement already satisfied: setuptools>=41.0.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from tensorboard) (61.2.0)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\nRequirement already satisfied: wheel>=0.26 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from tensorboard) (0.37.1)\nCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from tensorboard) (3.19.1)\nCollecting absl-py>=0.4\n  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\nRequirement already satisfied: grpcio>=1.24.3 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from tensorboard) (1.42.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from tensorboard) (2.0.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from tensorboard) (1.33.0)\nRequirement already satisfied: scipy>=1.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from seaborn) (1.7.3)\nRequirement already satisfied: pytz>=2020.1 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\nRequirement already satisfied: rsa<5,>=3.1.4 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\nRequirement already satisfied: six>=1.9.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.2)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\nRequirement already satisfied: idna<4,>=2.5 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\nInstalling collected packages: oauthlib, requests-oauthlib, torch, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, torchvision, torchmetrics, torchaudio, tensorboard\nSuccessfully installed absl-py-1.2.0 google-auth-oauthlib-0.4.6 oauthlib-3.2.1 requests-oauthlib-1.3.1 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torch-1.12.1 torchaudio-0.12.1 torchmetrics-0.10.0 torchvision-0.13.1\n",
  "history_begin_time" : 1665180984639,
  "history_end_time" : 1665181356984,
  "history_notes" : null,
  "history_process" : "39ur7y",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mc0cdb0rflb",
  "history_input" : "#eddy dependencies\ndef start_axes(title):\n    fig = plt.figure(figsize=(13, 5))\n    ax = fig.add_axes([0.03, 0.03, 0.90, 0.94])\n    ax.set_aspect(\"equal\")\n    ax.set_title(title, weight=\"bold\")\n    return ax\n\n\ndef update_axes(ax, mappable=None):\n    ax.grid()\n    if mappable:\n        plt.colorbar(mappable, cax=ax.figure.add_axes([0.94, 0.05, 0.01, 0.9]))\n\n\ndef plot_variable(grid_object, var_name, ax_title, **kwargs):\n    ax = start_axes(ax_title)\n    m = grid_object.display(ax, var_name, **kwargs)\n    update_axes(ax, m)\n    ax.set_xlim(grid_object.x_c.min(), grid_object.x_c.max())\n    ax.set_ylim(grid_object.y_c.min(), grid_object.y_c.max())\n    return ax, m",
  "history_output" : "",
  "history_begin_time" : 1665180985321,
  "history_end_time" : 1665181356992,
  "history_notes" : null,
  "history_process" : "uolls4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jyz28rkfgri",
  "history_input" : "#Data utils code\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom matplotlib.animation import ArtistAnimation\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\n\ntorch.manual_seed(42)\n\n\ndef get_eddy_dataloader(\n    files, binary=False, transform=None, batch_size=32, shuffle=True, val_split=0\n):\n    \"\"\"\n    Given a list of npz files, return dataloader(s) for train (and val).\n    Args:\n        files (list) : list of npz files\n        binary (bool) : whether to use binary masks or not.\n                        If True, treat cyclonic and anticyclonic eddies as single positive class.\n        transform (callable) : optional transform to be applied on a sample.\n        batch_size (int) : batch size for dataloader\n        shuffle (bool) : whether to shuffle the dataset or not\n        val_split (float) : fraction of data to be used as validation set.\n                            If 0, no validation split is performed.\n    Returns:\n        (train_loader, val_loader) if val_split > 0; (train_loader, None) otherwise\n    \"\"\"\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n    loader_kwargs = dict(batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n    if val_split > 0:\n        train_ds, val_ds = ds\n        train_dl = DataLoader(train_ds, **loader_kwargs)\n        val_dl = DataLoader(val_ds, **loader_kwargs)\n    else:\n        train_dl = DataLoader(ds, **loader_kwargs)\n        val_dl = None\n    return train_dl, val_dl\n\n\ndef get_eddy_dataset(files, binary=None, transform=None, val_split=0):\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n    print(f\"Read {len(masks)} samples from {files}.\")\n    if val_split > 0:\n        # split into training and validation sets (80% training, 20% validation)\n        train_idx, val_idx = train_test_split(\n            np.arange(len(masks)), test_size=val_split, random_state=42\n        )\n        train_ds = EddyDataset(\n            masks[train_idx],\n            var_filtered[train_idx],\n            dates[train_idx],\n            transform=transform,\n            binary_mask=binary,\n        )\n\n        val_ds = EddyDataset(\n            masks[val_idx],\n            var_filtered[val_idx],\n            dates[val_idx],\n            transform=transform,\n            binary_mask=binary,\n        )\n    else:\n        train_ds = EddyDataset(\n            masks, var_filtered, dates, transform=transform, binary_mask=binary\n        )\n        val_ds = None\n    return train_ds, val_ds\n\n\ndef read_npz_files(npz_files: list):\n    \"\"\"Load a list of npz files, concatenate, and return separate arrays for eddy segmentation\"\"\"\n    # load npz file into separate variables\n    if isinstance(npz_files, str):\n        npz_files = [npz_files]\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n    masks, dates, var, var_filtered, lon_subset, lat_subset = eddy_dict_to_vars(\n        npz_contents\n    )\n    return masks, dates, var, var_filtered, lon_subset, lat_subset, npz_contents\n\n\ndef eddy_dict_to_vars(npz_contents):\n    masks = np.concatenate(\n        [npz_content[\"masks\"] for npz_content in npz_contents], axis=0\n    )\n    dates = np.concatenate(\n        [npz_content[\"dates\"] for npz_content in npz_contents], axis=0\n    )\n    # var = np.concatenate([npz_content[\"var\"] for npz_content in npz_contents], axis=0)\n    var = None\n    var_filtered = np.concatenate(\n        [npz_content[\"var_filtered\"] for npz_content in npz_contents], axis=0\n    )\n    if \"lon_subset\" in npz_contents[0]:\n        lon_subset = np.concatenate(\n            [npz_content[\"lon_subset\"] for npz_content in npz_contents], axis=0\n        )\n        lat_subset = np.concatenate(\n            [npz_content[\"lat_subset\"] for npz_content in npz_contents], axis=0\n        )\n    else:\n        lon_subset = lat_subset = None\n    return masks, dates, var, var_filtered, lon_subset, lat_subset\n\n\nclass EddyDataset(torch.utils.data.Dataset):\n    def __init__(self, masks, gv, dates, transform=None, binary_mask=False):\n        \"\"\"PyTorch dataset for eddy detection\n        Args:\n            masks (np.array): array of segmentation masks with shape: (N_dates, N_lon, N_lat)\n                Can have 3 values: 0, 1 and 2, where 1 = anticyclonic, 2 = cyclonic and 0 = no eddy\n            gv (np.array): array of GV maps with shape: (N_dates, N_lon, N_lat)\n                Example GVs: sea level anomaly, absolute dynamic topography\n            transform (callable, optional): Transformation to be applied on a sample.\n            binary_mask (bool, optional): If true, all eddies (anticyclonic and cyclonic) will be assigned a value of 1\n        \"\"\"\n        self.masks = masks\n        self.gv = gv.astype(np.float32)  # GV stands for Geophysical Variable\n        self.dates = dates\n        self.transform = transform\n        self.binary_mask = binary_mask\n\n    def __getitem__(self, index, return_date=True):\n        # return image and mask for a given index\n        image = self.gv[index, :, :].copy()\n        mask = self.masks[index, :, :].copy()\n        date = self.dates[index]\n\n        # transpose\n        image = image.T\n        mask = mask.T\n\n        # address regions of land that are represented as -2147483648\n        image[image < -10000] = 0\n\n        if image.ndim == 2:\n            image = np.expand_dims(image, axis=0)  # make ndim = 3\n\n        if self.transform:\n            image = self.transform(image)\n\n        # if image and mask are numpy arrays, convert them to torch tensors\n        if isinstance(image, np.ndarray):\n            image = torch.from_numpy(image)\n        if isinstance(mask, np.ndarray):\n            mask = torch.from_numpy(mask)\n\n        if self.binary_mask:\n            mask[mask >= 1] = 1\n\n        # convert to float\n        image = image.float()\n\n        if return_date:\n            # convert date to tensor\n            # date_str = date.strftime(\"%Y-%m-%d\")\n            # date =\n            return image, mask, index\n        else:\n            return image, mask\n\n    def __len__(self):\n        return self.masks.shape[0]\n\n    def plot_sample(self, N=5):\n\n        # var in first column, mask in second column\n        num_cols = 2\n        num_rows = N\n        fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4))\n        ax[0, 0].set_title(\"GV\")\n        ax[0, 1].set_title(\"Mask\")\n        for i in range(num_rows):\n            # get random sample from self\n            n = np.random.randint(0, len(self))\n            gv, mask, index = self.__getitem__(n, return_date=True)\n            gv = np.squeeze(gv.cpu().detach().numpy())\n            mask = np.squeeze(mask.cpu().detach().numpy())\n            date = self.dates[index].strftime(\"%Y-%m-%d\")\n            # ax[i, 0].pcolormesh(lon_subset, lat_subset, gv.T, cmap=\"RdBu_r\", vmin=-0.15, vmax=0.15)\n            ax[i, 0].imshow(gv, cmap=\"RdBu_r\", vmin=-0.15, vmax=0.15)\n            ax[i, 0].set_title(f\"GV ({date})\")\n            ax[i, 0].axis(\"off\")\n            ax[i, 1].imshow(mask, cmap=\"viridis\")\n            ax[i, 1].set_title(f\"Mask ({date})\")\n            ax[i, 1].axis(\"off\")\n\n    def animate(self):\n        fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n        print(f\"Drawing animation of GV and segmentation mask\")\n        artists = []\n        for i in tqdm(range(len(self)), desc=\"Animating eddies:\"):\n            gv, mask, date_idx = self.__getitem__(i, return_date=True)\n            date = self.dates[date_idx].strftime(\"%Y-%m-%d\")\n            im1 = ax[0].imshow(gv.squeeze(), cmap=\"RdBu_r\", vmin=-0.15, vmax=0.15)\n            t1 = ax[0].text(\n                0.5,\n                1.05,\n                f\"GV {date}\",\n                size=plt.rcParams[\"axes.titlesize\"],\n                ha=\"center\",\n                transform=ax[0].transAxes,\n            )\n            ax[0].axis(\"off\")\n\n            im2 = ax[1].imshow(mask.squeeze(), cmap=\"viridis\")\n            t2 = ax[1].text(\n                0.5,\n                1.05,\n                f\"Mask {date}\",\n                size=plt.rcParams[\"axes.titlesize\"],\n                ha=\"center\",\n                transform=ax[1].transAxes,\n            )\n            ax[1].axis(\"off\")\n            plt.tight_layout()\n            artists.append([im1, t1, im2, t2])\n            fig.canvas.draw()\n            fig.canvas.flush_events()\n        animation = ArtistAnimation(fig, artists, interval=500, blit=True)\n        plt.close()\n        return animation\n\ndef transform_ssh(ssh_array):\n    # normalize sea level anomaly between 0 and 1 based on min max\n    ssh_array = (ssh_array - ssh_array.min()) / (ssh_array.max() - ssh_array.min())\n    return ssh_array\n\n\n# convert npy to compressed npz\ndef convert_npy_to_npz(npy_file):\n    npz_file = npy_file.replace(\".npy\", \".npz\")\n    npy_contents = np.load(npy_file)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/jyz28rkfgri/data_utils.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1665180984628,
  "history_end_time" : 1665181356995,
  "history_notes" : null,
  "history_process" : "oc42ub",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cb0dot4dx2x",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/cb0dot4dx2x/eddynet.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1665180985320,
  "history_end_time" : 1665181356996,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tyhf9sodjrk",
  "history_input" : "#Eddy Train Utils\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchmetrics\nfrom torch.utils.tensorboard.summary import hparams\n\n\ndef run_batch(\n    model,\n    loss_fn,\n    x_batch,\n    y_batch,\n    opt=None,\n    sched=None,\n    metrics=None,\n    return_pred=False,\n):\n    \"\"\"Run a batch of data through the model and return loss and metrics.\"\"\"\n    if torch.cuda.is_available():\n        loss_fn = loss_fn.to(device=\"cuda\")\n        x_batch = x_batch.to(device=\"cuda\", non_blocking=True)\n        y_batch = y_batch.to(device=\"cuda\", non_blocking=True)\n\n    # forward pass\n    logits = model(x_batch)\n    if return_pred:\n        preds = logits.argmax(axis=1).squeeze()\n    # reshape so that each pixel in seg. mask can be treated as separate instance\n    mask_flattened, logits = reshape_mask_and_predictions(y_batch, logits)\n    # compute loss\n    loss = loss_fn(logits, mask_flattened)\n    # backward pass\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        if sched is not None:\n            sched.step()\n    # update metrics\n    if metrics is not None:\n        metrics.update(logits, mask_flattened)\n    if return_pred:\n        return loss.item(), preds\n    else:\n        return loss.item()\n\n\ndef reshape_mask_and_predictions(mask, prediction):\n    \"\"\"flatten mask and prediction in each batch\"\"\"\n    mask_reshaped = mask.flatten().to(torch.int64)\n    # pred_reshaped = prediction.flatten(start_dim=-2, end_dim=-1)\n    # logits shape: [B, C, 128, 128] -> [B, 128, 128, C] -> [B * 128 * 128, C]\n    pred_reshaped = prediction.permute((0, 2, 3, 1)).flatten(start_dim=0, end_dim=-2)\n    return mask_reshaped, pred_reshaped\n\n\ndef get_metrics(N, sync):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.F1Score(\n            average=\"micro\",\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        # torchmetrics.AUROC(dist_sync_on_step=sync, num_classes=N),\n        # StorePredLabel(dist_sync_on_step=sync),\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\n\ndef write_metrics_to_tensorboard(N, metrics, writer, epoch, train_or_val):\n    m = metrics.compute()\n    for k, v in m.items():\n        if k == \"StorePredLabel\":\n            pred, label = v\n            label = nn.functional.one_hot(label, N)\n            writer.add_pr_curve(f\"{train_or_val}/pr_curve\", label, pred, epoch)\n        # handle class-level metrics\n        elif isinstance(v, torch.Tensor) and len(v.shape) > 0 and v.shape[-1] > 1:\n            for i, v_ in enumerate(v):\n                if N == 2:  # binary\n                    l = \"negative\" if i == 0 else \"positive\"\n                elif N == 3:\n                    if i == 0:\n                        l = \"negative\"\n                    elif i == 1:\n                        l = \"anticyclonic\"\n                    elif i == 2:\n                        l = \"cyclonic\"\n                else:\n                    raise NotImplementedError(f\"{N} classes not supported\")\n                writer.add_scalar(f\"{train_or_val}/{k}_{l}\", v_, epoch)\n        else:\n            writer.add_scalar(f\"{train_or_val}/{k}\", v, epoch)\n    return m\n\n\ndef filter_scalar_metrics(metrics_dict):\n    \"\"\"Filters the output of metrics.compute() and returns only the scalar metrics.\"\"\"\n    output = {}\n    for k, v in metrics_dict.items():\n        if (isinstance(v, torch.Tensor) or isinstance(v, np.ndarray)) and len(\n            v.shape\n        ) == 0:\n            output[k] = v\n    return output\n\n\ndef add_hparams(\n    torch_tb_writer, hparam_dict, metric_dict, hparam_domain_discrete=None, epoch_num=0\n):\n    \"\"\"Add a set of hyperparameters to be compared in TensorBoard.\n    Args:\n        hparam_dict (dict): Each key-value pair in the dictionary is the\n            name of the hyper parameter and it's corresponding value.\n            The type of the value can be one of `bool`, `string`, `float`,\n            `int`, or `None`.\n        metric_dict (dict): Each key-value pair in the dictionary is the\n            name of the metric and it's corresponding value. Note that the key used\n            here should be unique in the tensorboard record. Otherwise the value\n            you added by ``add_scalar`` will be displayed in hparam plugin. In most\n            cases, this is unwanted.\n        hparam_domain_discrete: (Optional[Dict[str, List[Any]]]) A dictionary that\n            contains names of the hyperparameters and all discrete values they can hold\n    \"\"\"\n    torch._C._log_api_usage_once(\"tensorboard.logging.add_hparams\")\n    if type(hparam_dict) is not dict or type(metric_dict) is not dict:\n        raise TypeError(\"hparam_dict and metric_dict should be dictionary.\")\n    exp, ssi, sei = hparams(hparam_dict, metric_dict, hparam_domain_discrete)\n\n    torch_tb_writer.file_writer.add_summary(exp)\n    torch_tb_writer.file_writer.add_summary(ssi)\n    torch_tb_writer.file_writer.add_summary(sei)\n    for k, v in metric_dict.items():\n        torch_tb_writer.add_scalar(k, v, epoch_num)\n\n\n# Taken from: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\n    def __init__(\n        self,\n        patience=7,\n        verbose=False,\n        delta=0,\n        path=\"checkpoint.pt\",\n        min_epochs=30,\n    ):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.min_epochs = min_epochs\n        self.epochs = 0\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.counter >= self.patience and self.epochs >= self.min_epochs:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n        self.epochs += 1\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            self.trace_func(\n                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n            )\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/tyhf9sodjrk/eddy_train_utils.py\", line 3, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1665180986595,
  "history_end_time" : 1665181356998,
  "history_notes" : null,
  "history_process" : "bomi2j",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]
